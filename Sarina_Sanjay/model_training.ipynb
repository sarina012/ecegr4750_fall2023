{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #1 - The Classic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number [1/100] - Training Loss: 1325.538940, Validation Loss: 1030.392212\n",
      "Epoch Number [2/100] - Training Loss: 1055.930800, Validation Loss: 1064.283691\n",
      "Epoch Number [3/100] - Training Loss: 856.404335, Validation Loss: 769.828125\n",
      "Epoch Number [4/100] - Training Loss: 697.816964, Validation Loss: 674.327148\n",
      "Epoch Number [5/100] - Training Loss: 580.048274, Validation Loss: 379.456848\n",
      "Epoch Number [6/100] - Training Loss: 474.170437, Validation Loss: 373.378754\n",
      "Epoch Number [7/100] - Training Loss: 388.001859, Validation Loss: 324.836090\n",
      "Epoch Number [8/100] - Training Loss: 325.413252, Validation Loss: 262.487305\n",
      "Epoch Number [9/100] - Training Loss: 268.108595, Validation Loss: 172.812164\n",
      "Epoch Number [10/100] - Training Loss: 229.032202, Validation Loss: 156.908463\n",
      "Epoch Number [11/100] - Training Loss: 193.130687, Validation Loss: 114.643303\n",
      "Epoch Number [12/100] - Training Loss: 171.381532, Validation Loss: 132.949066\n",
      "Epoch Number [13/100] - Training Loss: 153.152680, Validation Loss: 88.147903\n",
      "Epoch Number [14/100] - Training Loss: 139.850754, Validation Loss: 112.416306\n",
      "Epoch Number [15/100] - Training Loss: 129.818900, Validation Loss: 96.973892\n",
      "Epoch Number [16/100] - Training Loss: 131.126310, Validation Loss: 70.943687\n",
      "Epoch Number [17/100] - Training Loss: 124.992721, Validation Loss: 69.083847\n",
      "Epoch Number [18/100] - Training Loss: 121.750739, Validation Loss: 58.610233\n",
      "Epoch Number [19/100] - Training Loss: 121.960926, Validation Loss: 48.999596\n",
      "Epoch Number [20/100] - Training Loss: 121.686263, Validation Loss: 96.141678\n",
      "Epoch Number [21/100] - Training Loss: 121.752186, Validation Loss: 73.160568\n",
      "Epoch Number [22/100] - Training Loss: 122.668595, Validation Loss: 74.805862\n",
      "Epoch Number [23/100] - Training Loss: 123.190616, Validation Loss: 91.809738\n",
      "Epoch Number [24/100] - Training Loss: 121.307611, Validation Loss: 51.002197\n",
      "Epoch Number [25/100] - Training Loss: 122.893730, Validation Loss: 39.565872\n",
      "Epoch Number [26/100] - Training Loss: 119.486179, Validation Loss: 74.411263\n",
      "Epoch Number [27/100] - Training Loss: 122.919774, Validation Loss: 65.176895\n",
      "Epoch Number [28/100] - Training Loss: 125.101211, Validation Loss: 69.526581\n",
      "Epoch Number [29/100] - Training Loss: 123.410245, Validation Loss: 52.088634\n",
      "Epoch Number [30/100] - Training Loss: 122.814255, Validation Loss: 98.227905\n",
      "Epoch Number [31/100] - Training Loss: 122.150716, Validation Loss: 99.641258\n",
      "Epoch Number [32/100] - Training Loss: 123.135210, Validation Loss: 93.639023\n",
      "Epoch Number [33/100] - Training Loss: 123.082474, Validation Loss: 51.771881\n",
      "Epoch Number [34/100] - Training Loss: 123.099249, Validation Loss: 69.519424\n",
      "Epoch Number [35/100] - Training Loss: 124.458742, Validation Loss: 108.338120\n",
      "Epoch Number [36/100] - Training Loss: 123.512765, Validation Loss: 76.440010\n",
      "Epoch Number [37/100] - Training Loss: 124.246576, Validation Loss: 67.464096\n",
      "Epoch Number [38/100] - Training Loss: 122.279447, Validation Loss: 105.651688\n",
      "Epoch Number [39/100] - Training Loss: 121.276333, Validation Loss: 115.582260\n",
      "Epoch Number [40/100] - Training Loss: 121.663015, Validation Loss: 73.938873\n",
      "Epoch Number [41/100] - Training Loss: 121.891802, Validation Loss: 91.836098\n",
      "Epoch Number [42/100] - Training Loss: 122.040746, Validation Loss: 70.525444\n",
      "Epoch Number [43/100] - Training Loss: 120.914445, Validation Loss: 64.533234\n",
      "Epoch Number [44/100] - Training Loss: 118.124349, Validation Loss: 97.265778\n",
      "Epoch Number [45/100] - Training Loss: 122.576235, Validation Loss: 90.374779\n",
      "Epoch Number [46/100] - Training Loss: 122.088904, Validation Loss: 67.377846\n",
      "Epoch Number [47/100] - Training Loss: 122.978745, Validation Loss: 91.791588\n",
      "Epoch Number [48/100] - Training Loss: 122.071033, Validation Loss: 72.603645\n",
      "Epoch Number [49/100] - Training Loss: 123.906305, Validation Loss: 75.164665\n",
      "Epoch Number [50/100] - Training Loss: 123.339741, Validation Loss: 74.761192\n",
      "Epoch Number [51/100] - Training Loss: 119.971958, Validation Loss: 57.163891\n",
      "Epoch Number [52/100] - Training Loss: 122.629775, Validation Loss: 62.463772\n",
      "Epoch Number [53/100] - Training Loss: 120.914890, Validation Loss: 75.003464\n",
      "Epoch Number [54/100] - Training Loss: 126.968234, Validation Loss: 62.747105\n",
      "Epoch Number [55/100] - Training Loss: 124.336351, Validation Loss: 62.111034\n",
      "Epoch Number [56/100] - Training Loss: 123.042822, Validation Loss: 35.881649\n",
      "Epoch Number [57/100] - Training Loss: 121.264779, Validation Loss: 72.326538\n",
      "Epoch Number [58/100] - Training Loss: 120.221172, Validation Loss: 50.950542\n",
      "Epoch Number [59/100] - Training Loss: 119.852798, Validation Loss: 91.965248\n",
      "Epoch Number [60/100] - Training Loss: 124.155635, Validation Loss: 81.514893\n",
      "Epoch Number [61/100] - Training Loss: 121.943620, Validation Loss: 54.116028\n",
      "Epoch Number [62/100] - Training Loss: 119.970298, Validation Loss: 94.031105\n",
      "Epoch Number [63/100] - Training Loss: 122.828946, Validation Loss: 113.017715\n",
      "Epoch Number [64/100] - Training Loss: 122.661999, Validation Loss: 77.582436\n",
      "Epoch Number [65/100] - Training Loss: 121.604735, Validation Loss: 65.368881\n",
      "Epoch Number [66/100] - Training Loss: 121.934478, Validation Loss: 78.069458\n",
      "Epoch Number [67/100] - Training Loss: 124.521535, Validation Loss: 106.108673\n",
      "Epoch Number [68/100] - Training Loss: 123.337955, Validation Loss: 96.298996\n",
      "Epoch Number [69/100] - Training Loss: 123.579021, Validation Loss: 57.147655\n",
      "Epoch Number [70/100] - Training Loss: 119.525316, Validation Loss: 61.320347\n",
      "Epoch Number [71/100] - Training Loss: 120.893773, Validation Loss: 67.579010\n",
      "Epoch Number [72/100] - Training Loss: 123.941945, Validation Loss: 56.199326\n",
      "Epoch Number [73/100] - Training Loss: 121.753483, Validation Loss: 85.436188\n",
      "Epoch Number [74/100] - Training Loss: 122.393001, Validation Loss: 85.551750\n",
      "Epoch Number [75/100] - Training Loss: 124.519095, Validation Loss: 43.045254\n",
      "Epoch Number [76/100] - Training Loss: 122.355154, Validation Loss: 72.177010\n",
      "Epoch Number [77/100] - Training Loss: 120.989619, Validation Loss: 71.179726\n",
      "Epoch Number [78/100] - Training Loss: 123.111277, Validation Loss: 70.793037\n",
      "Epoch Number [79/100] - Training Loss: 120.476929, Validation Loss: 76.032074\n",
      "Epoch Number [80/100] - Training Loss: 123.822194, Validation Loss: 77.878410\n",
      "Epoch Number [81/100] - Training Loss: 122.361776, Validation Loss: 75.044983\n",
      "Epoch Number [82/100] - Training Loss: 121.549361, Validation Loss: 71.605728\n",
      "Epoch Number [83/100] - Training Loss: 123.570330, Validation Loss: 93.556046\n",
      "Epoch Number [84/100] - Training Loss: 122.769297, Validation Loss: 63.066658\n",
      "Epoch Number [85/100] - Training Loss: 121.976758, Validation Loss: 70.887299\n",
      "Epoch Number [86/100] - Training Loss: 121.767977, Validation Loss: 60.893661\n",
      "Epoch Number [87/100] - Training Loss: 120.501572, Validation Loss: 91.212677\n",
      "Epoch Number [88/100] - Training Loss: 123.478587, Validation Loss: 60.599583\n",
      "Epoch Number [89/100] - Training Loss: 123.706412, Validation Loss: 94.237022\n",
      "Epoch Number [90/100] - Training Loss: 121.498395, Validation Loss: 86.989708\n",
      "Epoch Number [91/100] - Training Loss: 120.231548, Validation Loss: 75.625847\n",
      "Epoch Number [92/100] - Training Loss: 122.410141, Validation Loss: 67.667679\n",
      "Epoch Number [93/100] - Training Loss: 124.938561, Validation Loss: 58.305347\n",
      "Epoch Number [94/100] - Training Loss: 123.013138, Validation Loss: 50.145802\n",
      "Epoch Number [95/100] - Training Loss: 124.205757, Validation Loss: 95.394592\n",
      "Epoch Number [96/100] - Training Loss: 123.289057, Validation Loss: 73.310448\n",
      "Epoch Number [97/100] - Training Loss: 117.867113, Validation Loss: 74.073517\n",
      "Epoch Number [98/100] - Training Loss: 126.145256, Validation Loss: 92.319893\n",
      "Epoch Number [99/100] - Training Loss: 125.120262, Validation Loss: 51.667114\n",
      "Epoch Number [100/100] - Training Loss: 124.837175, Validation Loss: 39.641216\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from dataloader import CustomDataloader\n",
    "from networks import LinearRegressionModel\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/UTKFaceAugmented.csv')\n",
    "\n",
    "# Encode categorical variables (gender, race, age_range)\n",
    "label_encoder = LabelEncoder()\n",
    "df['gender'] = label_encoder.fit_transform(df['gender'])\n",
    "df['race'] = label_encoder.fit_transform(df['race'])\n",
    "df['age_range'] = label_encoder.fit_transform(df['age_range'])\n",
    "\n",
    "# Encode binary columns (has_tiktok, remembers_disco, uses_skincare)\n",
    "binary_columns = ['has_tiktok', 'remembers_disco', 'uses_skincare']\n",
    "df[binary_columns] = df[binary_columns].apply(lambda x: x.map({'yes': 1, 'no': 0}))\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Select features and target variable\n",
    "features = ['num_haircuts_life', 'has_tiktok', 'remembers_disco', 'uses_skincare', 'max_annual_earnings']\n",
    "X = df[features]\n",
    "y = df['age']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train_tensor, y_train_tensor = torch.Tensor(X_train), torch.Tensor(y_train.values)\n",
    "X_test_tensor, y_test_tensor = torch.Tensor(X_test), torch.Tensor(y_test.values)\n",
    "\n",
    "\n",
    "# Create CustomDataloader instances\n",
    "training_dataloader = CustomDataloader(x=X_train_tensor, y=y_train_tensor, batch_size=64, randomize=True)\n",
    "testing_dataloader = CustomDataloader(x=X_test_tensor, y=y_test_tensor, batch_size=64, randomize=False)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegressionModel(input_size=X_train.shape[1])\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Number of epochs\n",
    "total_epochs = 100\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for _ in range(len(training_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        train_batch = training_dataloader.fetch_batch()\n",
    "        X_batch, y_batch = train_batch['x_batch'], train_batch['y_batch']\n",
    "\n",
    "        predictions = model(X_batch)\n",
    "        training_loss = criterion(predictions.squeeze(), y_batch)\n",
    "\n",
    "        # Add L2 regularization\n",
    "        l2_regularization = 0.01\n",
    "        l2_loss = 0\n",
    "        for param in model.parameters():\n",
    "            l2_loss += torch.norm(param, p=2)\n",
    "        loss = training_loss + l2_regularization * l2_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    training_losses.append(total_loss / len(training_dataloader))\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_validation_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(len(testing_dataloader)):\n",
    "            validation_batch_set = testing_dataloader.fetch_batch()\n",
    "            X_validation_batch_set, y_validation_batch_set = validation_batch_set['x_batch'], validation_batch_set['y_batch']\n",
    "            predictions = model(X_validation_batch_set)\n",
    "            validation_loss = criterion(predictions.squeeze(), y_validation_batch_set)\n",
    "            total_validation_loss += validation_loss.item()\n",
    "\n",
    "    validation_losses.append(total_validation_loss / len(testing_dataloader))\n",
    "\n",
    "    print(f'Epoch Number [{epoch + 1}/{total_epochs}] - Training Loss: {training_losses[-1]:.6f}, Validation Loss: {validation_losses[-1]:.6f}')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "torch.save(training_losses, 'training_model_one.pt')\n",
    "torch.save(validation_losses, 'validation_model_one.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this code to calculate statistics on the test set\n",
    "\"\"\"\n",
    "predicted_age = []\n",
    "actual_age = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in testing_dataloader:\n",
    "        inputs, labels = batch['image'], batch['age']\n",
    "        outputs = model(inputs)\n",
    "        predicted_age.extend(outputs.cpu().numpy().flatten())\n",
    "        actual_age.extend(labels.cpu().numpy())\n",
    "\n",
    "predicted_age = np.array(predicted_age)\n",
    "actual_age = np.array(actual_age)\n",
    "\n",
    "torch.save(predicted_age, 'predicted_age_model_one.pt') \n",
    "torch.save(actual_age, 'actual_age_model_one.pt')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - A Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Training Loss: 326.673378, Validation Loss: 181.896944\n",
      "Epoch 2/15 - Training Loss: 228.971071, Validation Loss: 151.311580\n",
      "Epoch 3/15 - Training Loss: 210.231485, Validation Loss: 206.273746\n",
      "Epoch 4/15 - Training Loss: 186.824242, Validation Loss: 130.630297\n",
      "Epoch 5/15 - Training Loss: 178.893163, Validation Loss: 132.763902\n",
      "Epoch 6/15 - Training Loss: 168.986976, Validation Loss: 131.021248\n",
      "Epoch 7/15 - Training Loss: 162.327910, Validation Loss: 118.788414\n",
      "Epoch 8/15 - Training Loss: 151.167130, Validation Loss: 113.514016\n",
      "Epoch 9/15 - Training Loss: 149.961242, Validation Loss: 137.317859\n",
      "Epoch 10/15 - Training Loss: 145.951772, Validation Loss: 114.626364\n",
      "Epoch 11/15 - Training Loss: 139.700369, Validation Loss: 125.629638\n",
      "Epoch 12/15 - Training Loss: 129.121845, Validation Loss: 118.124087\n",
      "Epoch 13/15 - Training Loss: 131.864146, Validation Loss: 107.297155\n",
      "Epoch 14/15 - Training Loss: 123.556334, Validation Loss: 108.460568\n",
      "Epoch 15/15 - Training Loss: 122.018830, Validation Loss: 120.020669\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataloader import NeuralNetworkData\n",
    "from networks import SimpleCNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data_dir = 'data/images'\n",
    "image_dir = Path(data_dir)\n",
    "\n",
    "# Create a DataFrame with file paths and ages\n",
    "filepaths = pd.Series(list(image_dir.glob(r'**/*.jpg')), name='Filepath').astype(str)\n",
    "ages = pd.Series(filepaths.apply(lambda x: int(os.path.splitext(os.path.basename(x))[0].split('_')[0])), name='Age')\n",
    "images = pd.concat([filepaths, ages], axis=1).sample(frac=1.0, random_state=1).reset_index(drop=True)\n",
    "\n",
    "image_amount = 23700 # Use 5000 images to speed up training time\n",
    "image_df = images.sample(image_amount, random_state=1).reset_index(drop=True)\n",
    "train_df, val_df = train_test_split(image_df, test_size=0.2, shuffle=True, random_state=1)\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.25, shuffle=True, random_state=1)\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = NeuralNetworkData(train_df, transform=transform)\n",
    "val_dataset = NeuralNetworkData(val_df, transform=transform)\n",
    "test_dataset = NeuralNetworkData(test_df, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SimpleCNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "total_epochs = 15\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(total_epochs):  # Change the number of epochs as needed\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs, labels = batch['image'], batch['age']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs, labels = batch['image'], batch['age']\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels.view(-1, 1).float())\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    # Calculate average validation loss for the epoch\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{total_epochs} - Training Loss: {train_losses[-1]:.6f}, Validation Loss: {val_losses[-1]:.6f}')\n",
    "\n",
    "model.eval()\n",
    "torch.save(train_losses, 'training_model_two.pt')\n",
    "torch.save(val_losses, 'validation_model_two.pt')\n",
    "\n",
    "\n",
    "# Test the model on the test set\n",
    "model.eval()\n",
    "predicted_ages = []\n",
    "actual_ages = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs, labels = batch['image'], batch['age']\n",
    "        outputs = model(inputs)\n",
    "        predicted_ages.extend(outputs.numpy().flatten())\n",
    "        actual_ages.extend(labels.numpy())\n",
    "\n",
    "predicted_ages = np.array(predicted_ages)\n",
    "true_ages = np.array(actual_ages)\n",
    "torch.save(predicted_ages, 'predicted_age_model_two.pt') \n",
    "torch.save(actual_ages, 'actual_age_model_two.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 - A Multimodal Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[0;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 33\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     36\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Increase epochs\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import CustomImageDataset\n",
    "from networks import MultiModalCNN\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset and create DataLoader\n",
    "dataset = CustomImageDataset(csv_file='data/UTKFaceAugmented.csv', img_dir='data/images', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_features = len(dataset[0][1])\n",
    "model = MultiModalCNN(num_features, 1)  # 1 output since it's regression\n",
    "model.to(torch.device('gpu' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "model.to(torch.device('gpu' if torch.cuda.is_available() else 'cpu'))\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1  # Increase epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, features, labels in dataloader:\n",
    "        images, features, labels = images.to(device), features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, features)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    " \n",
    "# Validation phase\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        images, features, labels = images.to(device), features.to(device), labels.to(device)        \n",
    "        outputs = model(images, features)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Calculate average validation loss for the epoch\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{total_epochs} - Training Loss: {train_losses:.6f}, Validation Loss: {val_losses:.6f}')\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model_three.pt')\n",
    "\n",
    "# Save the model\n",
    "torch.save(avg_train_loss, 'training_model_three.pt')\n",
    "torch.save(avg_val_loss, 'validation_model_three.pt')\n",
    "\n",
    "# Model evaluation and plotting\n",
    "model.eval()\n",
    "actual_ages = []\n",
    "predicted_ages = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, features, labels in dataloader:\n",
    "        images, features, labels = images.to(device), features.to(device), labels.to(device)\n",
    "        outputs = model(images, features)\n",
    "        actual_ages.extend(labels.cpu().numpy())\n",
    "        predicted_ages.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "# Plotting actual and predicted ages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Move data to the specified device\n",
    "images, features, labels = images.to(device), features.to(device), labels.to(device)\n",
    "\n",
    "torch.save(predicted_ages, 'predicted_age_model_three.pt') \n",
    "torch.save(actual_ages, 'actual_age_model_three.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
